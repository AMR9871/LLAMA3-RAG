# LLAMA3 RAG - Question Answering System

A Retrieval-Augmented Generation (RAG) system powered by LLAMA3 that answers questions based on web content.

## Overview

This application uses a RAG architecture to provide accurate answers to questions by:
1. Loading and processing content from any web URL
2. Splitting text into manageable chunks
3. Creating embeddings with Nomic's embedding model
4. Retrieving relevant context based on semantic similarity
5. Generating answers using LLAMA3

## Features

- Web scraping with LangChain's WebBaseLoader
- Document chunking with RecursiveCharacterTextSplitter
- Vector embeddings with Ollama (Nomic-embed-text model)
- Vector storage with Chroma
- Text generation with LLAMA3
- User-friendly Gradio interface

## Requirements

- Python 3.8+
- Gradio
- LangChain
- Beautiful Soup (bs4)
- Chroma
- Ollama

## Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/llama3-rag.git
cd llama3-rag

# Install dependencies
pip install gradio langchain langchain_community bs4 chromadb ollama
```

## Running Ollama Locally

Before running the application, make sure you have Ollama installed and the required models available:

```bash
# Install Ollama (if not already installed)
# See https://ollama.ai/download for installation instructions

# Pull the required models
ollama pull llama3
ollama pull nomic-embed-text
```

## Usage

Run the application:

```bash
python app.py
```

Then access the Gradio interface at http://localhost:7860 in your web browser.

### How to Use:

1. Enter a URL containing the content you want to query
2. Type your question
3. Click "Submit" to get an answer generated by LLAMA3 based on the content

## How It Works

1. **Document Loading**: WebBaseLoader fetches and parses the webpage content
2. **Text Splitting**: Content is divided into chunks for efficient processing
3. **Embedding**: Text chunks are converted to vector embeddings
4. **Retrieval**: When a question is asked, semantically similar chunks are retrieved
5. **Generation**: LLAMA3 generates an answer based on the retrieved context

## Customization

You can modify the code to:
- Adjust chunk size and overlap in the text splitter
- Use different embedding models
- Change the LLM (replace LLAMA3 with another model)
- Customize the Gradio interface

## License

[MIT License](LICENSE)

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.
